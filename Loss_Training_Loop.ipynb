{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d448502-ec2b-449d-afb7-783773506de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 11:42:42.904412: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-19 11:42:42.942654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/scratch/sadegh/1269429/ipykernel_1128691/1090198234.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  submodel_predicted_mask = torch.sigmoid(torch.tensor(submodel_predicted_mask)).unsqueeze(0)  # Add batch dimension and apply sigmoid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point: (173, 264), IoU (Submodel): 0.8678, Focal Loss: 0.07372962683439255, Dice Loss: 0.6223226189613342\n",
      "Point: (655, 1036), IoU (Submodel): 0.2203, Focal Loss: 0.0745801329612732, Dice Loss: 0.629086971282959\n",
      "Point: (717, 1233), IoU (Submodel): 0.7989, Focal Loss: 0.0741911455988884, Dice Loss: 0.6124395728111267\n",
      "Point: (1155, 639), IoU (Submodel): 0.7939, Focal Loss: 0.09358198195695877, Dice Loss: 0.9802486896514893\n",
      "Point: (1944, 1453), IoU (Submodel): 0.7668, Focal Loss: 0.0748417004942894, Dice Loss: 0.6123348474502563\n",
      "Point: (1685, 430), IoU (Submodel): 0.8852, Focal Loss: 0.0689886137843132, Dice Loss: 0.5665760636329651\n",
      "Point: (1584, 327), IoU (Submodel): 0.4256, Focal Loss: 0.06988122314214706, Dice Loss: 0.5789762735366821\n",
      "Point: (735, 768), IoU (Submodel): 0.0063, Focal Loss: 0.07077154517173767, Dice Loss: 0.5904623866081238\n",
      "Point: (2147, 1375), IoU (Submodel): 0.1109, Focal Loss: 0.07686154544353485, Dice Loss: 0.6679875254631042\n",
      "Point: (1674, 433), IoU (Submodel): 0.8853, Focal Loss: 0.06898757070302963, Dice Loss: 0.5665767192840576\n",
      "Point: (1466, 912), IoU (Submodel): 0.0373, Focal Loss: 0.07982774078845978, Dice Loss: 0.7117058634757996\n",
      "Point: (841, 1055), IoU (Submodel): 0.0211, Focal Loss: 0.07984741032123566, Dice Loss: 0.7120411992073059\n",
      "Point: (1497, 1315), IoU (Submodel): 0.3788, Focal Loss: 0.09033124148845673, Dice Loss: 0.8392477631568909\n",
      "Point: (1168, 792), IoU (Submodel): 0.8950, Focal Loss: 0.09338287264108658, Dice Loss: 0.9743214845657349\n",
      "Point: (1682, 1043), IoU (Submodel): 0.0203, Focal Loss: 0.07984840869903564, Dice Loss: 0.7120581865310669\n",
      "Point: (355, 2455), IoU (Submodel): 0.0129, Focal Loss: 0.09661885350942612, Dice Loss: 0.809337317943573\n",
      "Point: (1391, 1362), IoU (Submodel): 0.8968, Focal Loss: 0.055131033062934875, Dice Loss: 0.4325244426727295\n",
      "Point: (22, 63), IoU (Submodel): 0.9107, Focal Loss: 0.05510231480002403, Dice Loss: 0.43205589056015015\n",
      "Point: (760, 276), IoU (Submodel): 0.7623, Focal Loss: 0.08578680455684662, Dice Loss: 0.8110234141349792\n",
      "Point: (1159, 137), IoU (Submodel): 0.8914, Focal Loss: 0.055030327290296555, Dice Loss: 0.4328398108482361\n",
      "Point: (877, 1582), IoU (Submodel): 0.4582, Focal Loss: 0.07382622361183167, Dice Loss: 0.6264455914497375\n",
      "Point: (103, 1859), IoU (Submodel): 0.6005, Focal Loss: 0.07427629828453064, Dice Loss: 0.6221145391464233\n",
      "Point: (654, 1019), IoU (Submodel): 0.2167, Focal Loss: 0.09437848627567291, Dice Loss: 0.9985317587852478\n",
      "Point: (286, 1341), IoU (Submodel): 0.0562, Focal Loss: 0.0755934789776802, Dice Loss: 0.6502183675765991\n",
      "Point: (1068, 1698), IoU (Submodel): 0.5273, Focal Loss: 0.0892942026257515, Dice Loss: 0.8384600877761841\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools import mask as coco_mask\n",
    "from transformers import SamModel, SamProcessor\n",
    "from raffm import RaFFM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the original SAM model and processor\n",
    "original_model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(\"cuda\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "# RaFFM configuration and submodel initialization\n",
    "elastic_config = {\n",
    "    \"atten_out_space\": [1280],\n",
    "    \"inter_hidden_space\": [2048],\n",
    "    \"residual_hidden_space\": [2048],\n",
    "}\n",
    "raffm_model = RaFFM(original_model, elastic_config=elastic_config)\n",
    "submodel, params, config = raffm_model.random_resource_aware_model()\n",
    "submodel = submodel.to(\"cuda\")  # Move submodel to GPU\n",
    "\n",
    "models = {\"submodel\": submodel, \"original_model\": original_model}\n",
    "\n",
    "def get_image_info(dataset_directory, num_images=1):\n",
    "    image_mask_pairs = []\n",
    "    for filename in os.listdir(dataset_directory):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(dataset_directory, filename)\n",
    "            mask_filename = filename.replace(\".jpg\", \".json\")\n",
    "            mask_path = os.path.join(dataset_directory, mask_filename)\n",
    "            if os.path.exists(mask_path):\n",
    "                image_mask_pairs.append((image_path, mask_path))\n",
    "    selected_pairs = random.sample(image_mask_pairs, min(num_images, len(image_mask_pairs)))\n",
    "    return selected_pairs\n",
    "\n",
    "def get_ground_truth_masks(mask_path):\n",
    "    binary_masks = []\n",
    "    with open(mask_path, 'r') as json_file:\n",
    "        mask_data = json.load(json_file)\n",
    "    for annotation in mask_data['annotations']:\n",
    "        rle_mask = annotation['segmentation']\n",
    "        binary_mask = coco_mask.decode(rle_mask)\n",
    "        binary_masks.append(binary_mask)\n",
    "    return binary_masks\n",
    "\n",
    "def calculate_metrics(pred_mask, gt_mask):\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    iou = intersection / union if union != 0 else 0\n",
    "    return iou\n",
    "\n",
    "def valid_points_from_masks(gt_masks):\n",
    "    points = []\n",
    "    for mask in gt_masks:\n",
    "        ys, xs = np.where(mask > 0)\n",
    "        points += [(x, y) for x, y in zip(xs, ys)]\n",
    "    return points\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "    F_loss = alpha * (1-pt)**gamma * BCE_loss\n",
    "    if reduction == 'mean':\n",
    "        return F_loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return F_loss.sum()\n",
    "    else:\n",
    "        return F_loss\n",
    "\n",
    "def dice_loss(inputs, targets, smooth=1e-6):\n",
    "    inputs = torch.sigmoid(inputs)\n",
    "    inputs = inputs.reshape(-1)\n",
    "    targets = targets.reshape(-1)\n",
    "    intersection = (inputs * targets).sum()\n",
    "    dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "dataset_directory = \"SA1B\"\n",
    "selected_images = get_image_info(dataset_directory, num_images=5)\n",
    "\n",
    "# Process each selected image and its corresponding mask\n",
    "for image_path, mask_path in selected_images:\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    ground_truth_masks = get_ground_truth_masks(mask_path)\n",
    "    valid_points = valid_points_from_masks(ground_truth_masks)\n",
    "    \n",
    "    for _ in range(5):  # Process 5 random points from valid areas\n",
    "        if not valid_points:\n",
    "            continue\n",
    "        input_point = random.choice(valid_points)\n",
    "        raw_image = np.array(original_image)\n",
    "        relevant_gt_mask = next((mask for mask in ground_truth_masks if mask[input_point[1], input_point[0]] > 0), None)\n",
    "        if relevant_gt_mask is None:\n",
    "            continue\n",
    "        \n",
    "        input_points = [[input_point]]\n",
    "        inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            submodel_outputs = models[\"submodel\"](**inputs)\n",
    "            submodel_masks = processor.image_processor.post_process_masks(\n",
    "                submodel_outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n",
    "            )\n",
    "        \n",
    "        submodel_predicted_mask = submodel_masks[0].squeeze(0).squeeze(0).cpu()[1] if submodel_masks else None\n",
    "        \n",
    "        # Calculate IoU, focal loss, and Dice loss\n",
    "        if submodel_predicted_mask is not None and relevant_gt_mask is not None:\n",
    "            iou_submodel = calculate_metrics(submodel_predicted_mask, relevant_gt_mask)\n",
    "            submodel_predicted_mask = torch.sigmoid(torch.tensor(submodel_predicted_mask)).unsqueeze(0)  # Add batch dimension and apply sigmoid\n",
    "            relevant_gt_mask_tensor = torch.tensor(relevant_gt_mask, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "            fl_loss = focal_loss(submodel_predicted_mask, relevant_gt_mask_tensor)\n",
    "            dl_loss = dice_loss(submodel_predicted_mask, relevant_gt_mask_tensor)\n",
    "            print(f\"Point: {input_point}, IoU (Submodel): {iou_submodel:.4f}, Focal Loss: {fl_loss.item()}, Dice Loss: {dl_loss.item()}\")\n",
    "        else:\n",
    "            print(f\"No metrics calculated for the point {input_point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5715676d-b318-46dc-8c17-c0360ae04343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1567\n",
      "Epoch 2/10, Loss: 1.0052\n",
      "Epoch 3/10, Loss: 0.9932\n",
      "Epoch 4/10, Loss: 0.9910\n",
      "Epoch 5/10, Loss: 0.9888\n",
      "Epoch 6/10, Loss: 0.9898\n",
      "Epoch 7/10, Loss: 0.9897\n",
      "Epoch 8/10, Loss: 0.9927\n",
      "Epoch 9/10, Loss: 0.9899\n",
      "Epoch 10/10, Loss: 0.9895\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools import mask as coco_mask\n",
    "from transformers import SamModel, SamProcessor\n",
    "from raffm import RaFFM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'\n",
    "\n",
    "\n",
    "# Initialize the original SAM model and processor\n",
    "original_model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(\"cuda\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "# RaFFM configuration and submodel initialization\n",
    "elastic_config = {\n",
    "    \"atten_out_space\": [1280],\n",
    "    \"inter_hidden_space\": [2048],\n",
    "    \"residual_hidden_space\": [2048],\n",
    "}\n",
    "raffm_model = RaFFM(original_model, elastic_config=elastic_config)\n",
    "submodel, params, config = raffm_model.random_resource_aware_model()\n",
    "submodel = submodel.to(\"cuda\")  # Move submodel to GPU\n",
    "\n",
    "def get_image_info(dataset_directory, num_images=1):\n",
    "    image_mask_pairs = []\n",
    "    for filename in os.listdir(dataset_directory):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(dataset_directory, filename)\n",
    "            mask_filename = filename.replace(\".jpg\", \".json\")\n",
    "            mask_path = os.path.join(dataset_directory, mask_filename)\n",
    "            if os.path.exists(mask_path):\n",
    "                image_mask_pairs.append((image_path, mask_path))\n",
    "    selected_pairs = random.sample(image_mask_pairs, min(num_images, len(image_mask_pairs)))\n",
    "    return selected_pairs\n",
    "\n",
    "def get_ground_truth_masks(mask_path):\n",
    "    binary_masks = []\n",
    "    with open(mask_path, 'r') as json_file:\n",
    "        mask_data = json.load(json_file)\n",
    "    for annotation in mask_data['annotations']:\n",
    "        rle_mask = annotation['segmentation']\n",
    "        binary_mask = coco_mask.decode(rle_mask)\n",
    "        binary_masks.append(binary_mask)\n",
    "    return binary_masks\n",
    "\n",
    "def calculate_metrics(pred_mask, gt_mask):\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    iou = intersection / union if union != 0 else 0\n",
    "    return iou\n",
    "\n",
    "def valid_points_from_masks(gt_masks):\n",
    "    points = []\n",
    "    for mask in gt_masks:\n",
    "        ys, xs = np.where(mask > 0)\n",
    "        points += [(x, y) for x, y in zip(xs, ys)]\n",
    "    return points\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "    F_loss = alpha * (1-pt)**gamma * BCE_loss\n",
    "    if reduction == 'mean':\n",
    "        return F_loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return F_loss.sum()\n",
    "    else:\n",
    "        return F_loss\n",
    "\n",
    "def dice_loss(inputs, targets, smooth=1e-6):\n",
    "    inputs = torch.sigmoid(inputs)\n",
    "    inputs = inputs.reshape(-1)\n",
    "    targets = targets.reshape(-1)\n",
    "    intersection = (inputs * targets).sum()\n",
    "    dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "# Define loss functions\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "    F_loss = alpha * (1 - pt) ** gamma * BCE_loss\n",
    "    return F_loss.mean() if reduction == 'mean' else F_loss.sum()\n",
    "\n",
    "def dice_loss(inputs, targets, smooth=1e-6):\n",
    "    inputs = torch.sigmoid(inputs)\n",
    "    inputs = inputs.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    intersection = (inputs * targets).sum()\n",
    "    dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class SA1BDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_directory):\n",
    "        self.image_mask_pairs = get_image_info(dataset_directory, num_images=20)  # Adjust num_images as needed\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((1024, 1024)),  # Resize images to 1024x1024\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization for ImageNet\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mask_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path = self.image_mask_pairs[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = get_ground_truth_masks(mask_path)[0]  # Assuming there is at least one mask per image\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        mask = transforms.functional.resize(mask, (1024, 1024), interpolation=transforms.InterpolationMode.NEAREST)  # Resize mask\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "dataset = SA1BDataset(\"SA1B\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)  # Batch size set to 1 for simplicity\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(submodel.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "accumulation_steps = 4  # Adjust based on your memory constraints\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    submodel.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(\"cuda\")\n",
    "        masks = masks.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = submodel(images)['pred_masks']\n",
    "        pred_masks = outputs.squeeze(1)  # Remove the extra dimension\n",
    "\n",
    "        # Select the appropriate channel and squeeze the channel dimension\n",
    "        pred_masks = pred_masks[:, 0, :, :].unsqueeze(1)  # Assuming the first channel is the class of interest\n",
    "\n",
    "        # Resize predicted masks to match target masks\n",
    "        pred_masks_resized = torch.nn.functional.interpolate(pred_masks, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss_focal = focal_loss(pred_masks_resized, masks)\n",
    "        loss_dice = dice_loss(pred_masks_resized, masks)\n",
    "        loss = loss_focal + loss_dice\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb032ab-c7a6-4d9d-9a97-312ce96593d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_submodel.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_path = \"trained_submodel.pth\"\n",
    "torch.save(submodel.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe97f5-0040-4307-ad13-80565d991af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
